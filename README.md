# RAG Evaluations

Welcome to my exploration of Retrieval-Augmented Generation (RAG) evaluation techniques and chunking strategies. This site documents my journey understanding how different approaches to document chunking affect RAG system performance.

Start with the [Introduction](intro.md) to begin exploring!

![[rag-evals.png]]

## Quick Links

- [Chunking Strategies](rag-chunking-strategies.md)
- [Evaluation with RAGAS](rag-evaluation-with-ragas.md)
- [Building with LangGraph](rag-with-langgraph.md)
- [Results Comparison](rag-chunking-evaluation-results.md)
- [Audio Overview](rag-audio-overview.md)

## Overview

The site provides a walkthrough of:

- **RAG Chunking Strategies**: Comparing naive and semantic chunking approaches
- **Evaluation with RAGAS**: Using specialized metrics for RAG assessment
- **Result Comparisons**: Analyzing the impact of different chunking strategies
- **Best Practices**: Recommendations for implementing effective RAG systems
- **Audio Transcript**: Sample transcript to tease core concepts for a blog -- and motivate your audience to learn more

## Key Findings

- Semantic chunking significantly improves context precision and answer quality
- Vive la difference!  Your selection of chunking strategy should vary based on the type of queries performed
- Trade-offs exist between processing speed and retrieval quality
- Specialized evaluation frameworks like RAGAS provide insights traditional metrics miss

## License

This content is available under the MIT License.
